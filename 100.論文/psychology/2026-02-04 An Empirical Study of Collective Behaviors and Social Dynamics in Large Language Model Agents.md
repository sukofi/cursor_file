---
title: "An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents"
date: 2026-02-04
tags: [大規模言語モデル, 社会的相互作用, 有害言語検出, ソーシャルネットワーク, 偏向防止]
url: http://arxiv.org/abs/2602.03775v1
---

# An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents

## 基本情報
- **著者**: Farnoosh Hashemi, Michael W. Macy
- **文献URL**: [http://arxiv.org/abs/2602.03775v1](http://arxiv.org/abs/2602.03775v1)
- **取得日**: 2026-02-04
- **タグ**: 大規模言語モデル, 社会的相互作用, 有害言語検出, ソーシャルネットワーク, 偏向防止

## アブストラクト（日本語訳）
大規模言語モデル（LLM）は、社会的、文化的、政治的な相互作用においてますます重要な役割を果たしています。これらは人間の行動や意思決定の一部をシミュレート可能ですが、LLM同士の繰り返しの相互作用が、その偏りを増幅したり排他的な行動を引き起こしたりするかどうかは、まだ十分に検証されていません。本研究では、LLM駆動型のソーシャルメディアプラットフォームであるChirper.aiを用い、1年間にわたる32000体のLLMエージェント間の700万投稿と相互作用を分析しました。まず、同質性と社会的影響という基本的な社会現象が、人間と同様にLLMのソーシャルネットワークにも存在することを確認しました。次に、LLMの有害言語、その言語的特徴、および相互作用パターンを調査し、有害投稿においてLLMは人間とは異なる構造的パターンを示すことを発見しました。さらに、投稿におけるイデオロギー的傾向やコミュニティ内の分極化を検討し、潜在的な有害な活動を防止する方法に焦点を当てました。そこで、LLMエージェントに有害な投稿を避けるよう促す「Chain of Social Thought（CoST）」というシンプルかつ効果的な手法を提案します。

## 原文（Abstract）
Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.
