# An Empirical Study of Collective Behaviors and Social Dynamics in Large Language Model Agents

## 基本情報
- **著者**: Farnoosh Hashemi, Michael W. Macy
- **文献URL**: [http://arxiv.org/abs/2602.03775v1](http://arxiv.org/abs/2602.03775v1)
- **取得日**: 2026-02-04

## アブストラクト（日本語訳）
大型言語モデル（LLMs）はますます私たちの社会的、文化的、政治的な相互作用を媒介する存在となっています。これらは人間の行動や意思決定の一部を模倣することができますが、LLM同士の繰り返しの交流が偏見を増幅させたり排他的な行動を引き起こしたりするかどうかは十分に解明されていません。そこで本研究では、LLM駆動型のソーシャルメディアプラットフォームであるChirper.aiを対象に、1年間にわたり32,000体のLLMエージェント間で交わされた700万件の投稿と相互作用を分析しました。まず、ホモフィリー（類似性志向）と社会的影響について調査し、人間の社会ネットワークと類似した基本的現象がLLM間でも観察されることを明らかにしました。次に、LLMによる有害言語の特徴とその言語的側面、相互作用のパターンを分析した結果、人間とは異なる構造的パターンが見られることが判明しました。さらに、LLMの投稿におけるイデオロギー的傾向とコミュニティの分極化を調査し、その潜在的な有害行動の抑止に焦点を当てました。最後に、LLMエージェントに有害な投稿を控えるよう促す、Chain of Social Thought（CoST）と呼ばれるシンプルかつ効果的な手法を提案します。

## 原文（Abstract）
Large Language Models (LLMs) increasingly mediate our social, cultural, and political interactions. While they can simulate some aspects of human behavior and decision-making, it is still underexplored whether repeated interactions with other agents amplify their biases or lead to exclusionary behaviors. To this end, we study Chirper.ai-an LLM-driven social media platform-analyzing 7M posts and interactions among 32K LLM agents over a year. We start with homophily and social influence among LLMs, learning that similar to humans', their social networks exhibit these fundamental phenomena. Next, we study the toxic language of LLMs, its linguistic features, and their interaction patterns, finding that LLMs show different structural patterns in toxic posting than humans. After studying the ideological leaning in LLMs posts, and the polarization in their community, we focus on how to prevent their potential harmful activities. We present a simple yet effective method, called Chain of Social Thought (CoST), that reminds LLM agents to avoid harmful posting.
