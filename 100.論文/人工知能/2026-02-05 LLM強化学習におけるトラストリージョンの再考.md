# LLM強化学習におけるトラストリージョンの再考

## 元タイトル
Rethinking the Trust Region in LLM Reinforcement Learning

## 概要
強化学習（Reinforcement Learning，RL）は、大規模言語モデル（Large Language Models，LLMs）のファインチューニングにおいて基盤技術となっており、その標準的なアルゴリズムとして近接方策最適化法（Proximal Policy Optimization，PPO）が広く用いられている。しかしながら、本稿では、PPOの中心的な比率クリッピング機構が、LLMsに固有の大規模語彙に対して構造的に不適切であると論じる。PPOは、サンプリングされたトークンの確率比率に基づいて方策の更新を制約しているが、これは真の方策発散の単一サンプルによるノイジーなモンテカルロ推定にすぎない。このため、低確率トークンの更新が過度に厳しくペナルティされる一方で、高確率トークンの潜在的に破滅的な変化は十分に抑制されず、学習効率と安定性の低下を招くという非最適な学習ダイナミクスが生じる。これに対処するために、本研究では発散近接方策最適化法（Divergence Proximal Policy Optimization，DPPO）を提案する。DPPOはヒューリスティックなクリッピングを、方策発散の直接推定（例えば全変動距離やKLダイバージェンス）に基づくより理論的根拠のある制約へと置き換える。大きなメモリ負荷を避けるために、必要な発散をほぼ追加コストなく捉える効率的な2値化およびTop-K近似を導入した。広範な実証的評価により、DPPOは既存手法と比較して訓練の安定性および効率性において優越し、RLに基づくLLMのファインチューニングのより堅牢な基盤を提供することを示した。

## 元の概要 (Abstract)
Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.

## 論文情報
- **テーマ**: 人工知能
- **公開日**: 2026-02-04T18:59:04Z
- **PDF URL**: https://arxiv.org/pdf/2602.04879v1
- **取得日**: 2026-02-05

---
*このドキュメントは自動生成・翻訳されました。*
